[//]: # (
 
    
    )





### On the Kolmogorov-Arnold Networks

A novel deep learning architecture


LI Shaun, 2024-05-17
<!-- .element: style="font-size:20pt" -->

=== 

![alt text](2024-05-15-KAN/xzy.png) <!-- .element: width="350" -->

![alt text](2024-05-15-KAN/lzw.png) <!-- .element: width="350" -->


Media hype

==

<img class="r-stretch" src="2024-05-15-KAN/KAN-arxiv.png">

arxiv preprint and [follow-ups](https://arxiv.org/search/?query=Kolmogorov+kan&searchtype=all&source=header)

==

## [the pykan repo and follow-ups](https://github.com/search?q=Kolmogorov-Arnold&type=repositories&p=1)


==
<img class="r-stretch" src="2024-05-15-KAN/ziming.png">
Liu Ziming (刘子鸣) presents KANs on Swarma Pattern
==


![alt text](2024-05-15-KAN/wechat.png) <!-- .element: width="450" -->

![alt text](2024-05-15-KAN/datawhale.png) <!-- .element: width="450" -->

Swarma Pattern's most popular live stream in history

==

<img class="r-stretch" src="2024-05-15-KAN/ziming-songyuan.png">

6K+ Page Views


===

## Outline
- MLP
    - Multilayer Perceptron 
    - Universal Approximation Theorem 
- KAN
    - Kolmogorov-Arnold Representation Theorem 
    - Kolmogorov-Arnold Network 

===

Shallow Multilayer Perceptron
![alt text](2024-05-15-KAN/shallow-mlp.png)

How many learnable weights?

==
![alt text](2024-05-15-KAN/gpt4.png)<!-- .element: width="550" -->

GPT-4
==
![alt text](2024-05-15-KAN/gpt-4o.png)<!-- .element: width="500" -->

GPT-4o



==
Deep Multilayer Perceptron
![alt text](2024-05-15-KAN/deep-mlp.png)
$\text{MLP}(\mathbf{x}) = (W_3 \circ \sigma_2 \circ W_2 \circ \sigma_1 \circ W_1)(\mathbf{x})$
===

#### Universal Approximation Theorem

$$f(x) \approx \sum_{i=1}^{N(\epsilon)} a_i \sigma(w_i \cdot x + b_i)$$

[Visual Proof](http://neuralnetworksanddeeplearning.com/chap4.html)
==
#### Why deep networks?
UAT: Theoretical, Existence

Deep: Empirical, Constructive

===
![questions](https://media.licdn.com/dms/image/C4D12AQF2HyN6MILFGw/article-cover_image-shrink_720_1280/0/1646657644961?e=2147483647&v=beta&t=O7HBRXmp-4I1vnw3p8_2THSzNzPtA7cS76_5yrCfZrY)

===
#### Kolmogorov Arnold Representation Theorem
or Kolmogorov Arnold Superposition Theorem

===

![Kolmogorov](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Kolm_lect_prep.jpg/440px-Kolm_lect_prep.jpg) <!-- .element: height="350" -->
![Arnold](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTLwmdosTU2ITtb3Z2hp0cqTvIT-aGlhl1Wg6jqmbSVag&s) <!-- .element: height="350" -->

[A. N. Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov),   Vladimir Arnold
==
## [有多少个数学家叫 Kolmogorov?](https://www.zhihu.com/question/39613986/answer/125393050)
===
#### Kolmogorov Arnold Representation Theorem (KART)
$$f(x_1, \dots, x_n) = \sum_{q=1}^{2n+1} \Phi_q \left( \sum_{p=1}^n \phi_{q,p}(x_p) \right)$$
==


<img class="r-stretch" src="https://upload.wikimedia.org/wikipedia/commons/d/db/Alfred_North_Whitehead._Photograph._Wellcome_V0027330_%28cropped%29.jpg">

Alfred North Whitehead

*Civilization advances by extending the number of important operations which we can perform without thinking about them.*

==
#### Representation vs Superposition
$$f(x_1, \dots, x_n) = \sum_{q=1}^{2n+1} \Phi_q \left( \sum_{p=1}^n \phi_{q,p}(x_p) \right)$$

===

Shallow KAN
![alt text](2024-05-15-KAN/shallow-kan.png)
==
![alt text](2024-05-15-KAN/compare-shallow.png)
Compare Shallow MLP with Shallow KAN

===


#### Why Deep KAN

KART doesn't mean we have techniques for constructing a practical network.


$\phi_{q,p}(x_p)$ can be non-smooth and even fractal

==
#### Why Deep KAN

KART: Theoretical, Existence

Deep: Empirical, Constructive

==


<img class="r-stretch" src="2024-05-15-KAN/kart-to-kan.png">


$$\Phi = \(\phi_{q,p}\), \quad p = 1, 2, \dots, n_{\text{in}}, \quad q = 1, 2, \dots, n_{\text{out}}$$

==
<img class="r-stretch" src="2024-05-15-KAN/deepkan.png">
$$\text{KAN}(\mathbf{x}) = (\Phi_3 \circ \Phi_2 \circ \Phi_1)(\mathbf{x})$$
==
<img class="r-stretch" src="2024-05-15-KAN/mlp-kan.png">

===
![alt text](2024-05-15-KAN/b-splines.png)
An activation function is parameterized as a B-spline
#Params $= O(N^2L(G+k))$
===
#### Properties of KANs
- faster scaling
- interpretable

==
#### Faster scaling
![alt text](2024-05-15-KAN/symbolic-formulas.png)
Fitting symbolic formulas
==

#### Interpretable
![alt text](2024-05-15-KAN/interpretable.png)
==


Demo: [Hello Kan](http://localhost:8888/notebooks/hellokan.ipynb)
==

*We propose KANs as promising alternatives to MLPs*

Compare: *alternatives*, *replacements*



===
### Discussion
==

<img class="r-stretch" src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Henri_Poincar%C3%A9-2.jpg/220px-Henri_Poincar%C3%A9-2.jpg">

Henri Poincaré

*If we wish to foresee the future of mathematics our proper course is to study the history and present condition of the science.*

==
![alt text](2024-05-15-KAN/history.png)
==

![alt text](2024-05-15-KAN/reductionism-holism.png)

==

![alt text](2024-05-15-KAN/controversial-opinion.png)
Ziming's controverisal opinion

===
![questions](https://media.licdn.com/dms/image/C4D12AQF2HyN6MILFGw/article-cover_image-shrink_720_1280/0/1646657644961?e=2147483647&v=beta&t=O7HBRXmp-4I1vnw3p8_2THSzNzPtA7cS76_5yrCfZrY)